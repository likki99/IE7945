{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70ea1c923c8341eb9a26ba31f5907c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be8b7014e7c64f11a5e53079922b0d24",
              "IPY_MODEL_ae4e139583e843a7bdc89cb06252d34f",
              "IPY_MODEL_835b57942dd049baba06f85d27cda958"
            ],
            "layout": "IPY_MODEL_070e305242cd43b8aafa46e3546999e4"
          }
        },
        "be8b7014e7c64f11a5e53079922b0d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162af09a7d2a4925824ba0e3222daae8",
            "placeholder": "​",
            "style": "IPY_MODEL_f740f79e393348adac81468031540207",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ae4e139583e843a7bdc89cb06252d34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2946d111665c4a73b572b95b4f70ecd1",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3df5b81db37b459cbaa23e5f7ca44531",
            "value": 10
          }
        },
        "835b57942dd049baba06f85d27cda958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a49ae0adfbf4fa0ba86a2110706549f",
            "placeholder": "​",
            "style": "IPY_MODEL_c584d98873f44bfda729c4da1e9cacdf",
            "value": " 10/10 [01:45&lt;00:00, 10.66s/it]"
          }
        },
        "070e305242cd43b8aafa46e3546999e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162af09a7d2a4925824ba0e3222daae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f740f79e393348adac81468031540207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2946d111665c4a73b572b95b4f70ecd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df5b81db37b459cbaa23e5f7ca44531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a49ae0adfbf4fa0ba86a2110706549f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c584d98873f44bfda729c4da1e9cacdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install faker\n",
        "!pip uninstall pandas\n",
        "!pip install pandas==1.3.5\n",
        "!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "!autotrain setup --colab > setup_logs.txt\n",
        "!pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEtsR2rkLoIq",
        "outputId": "5d93a517-4a99-4327-c54e-5d0cc0bb3af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-25.9.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-25.9.1\n",
            "Found existing installation: pandas 2.0.3\n",
            "Uninstalling pandas-2.0.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/pandas-2.0.3.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/pandas/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled pandas-2.0.3\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Collecting torch\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting triton==2.3.1 (from torch)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.0\n",
            "    Uninstalling torch-2.2.0:\n",
            "      Successfully uninstalled torch-2.2.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n",
            "xformers 0.0.24 requires torch==2.2.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.1 torchvision-0.18.1 triton-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIAq7q-0jh6y",
        "outputId": "c5032829-25a3-4365-dbdc-47e1d4c355de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autotrain-advanced 0.7.129 requires pandas==2.2.2, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 1.8.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "statsmodels 0.14.2 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt9ev2QU5j5j"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "from faker import Faker\n",
        "\n",
        "faker = Faker()\n",
        "\n",
        "# Possible body parts and scan types\n",
        "body_parts = [\"Head\", \"Chest\", \"Abdomen\", \"Pelvis\", \"Spine\", \"Knee\", \"Shoulder\", \"Elbow\", \"Ankle\", \"Foot\"]\n",
        "scan_types = [\"CT\", \"MR\"]\n",
        "\n",
        "def introduce_noise(text):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    text = list(text)\n",
        "    num_noises = random.randint(1, max(1, len(text) // 10))  # Introduce noise in about 10% of the characters\n",
        "    for _ in range(num_noises):\n",
        "        pos = random.randint(0, len(text) - 1)\n",
        "        text[pos] = random.choice(string.ascii_letters + string.digits + string.punctuation + \" \")\n",
        "    return ''.join(text)\n",
        "\n",
        "# Function to generate random records\n",
        "def generate_random_record():\n",
        "    name = introduce_noise(faker.name() if random.choice([True, False]) else None)\n",
        "    date_1 = introduce_noise(faker.date_between(start_date='-30y', end_date='today').strftime('%m.%d.%Y') if random.choice([True, False]) else None)\n",
        "    dob = introduce_noise(faker.date_of_birth(minimum_age=18, maximum_age=90).strftime('%m.%d.%Y') if random.choice([True, False]) else None)\n",
        "    body_part = introduce_noise(random.choice(body_parts) if random.choice([True, False]) else None)\n",
        "    scan_type = introduce_noise(random.choice(scan_types) if random.choice([True, False]) else None)\n",
        "\n",
        "    input_record = []\n",
        "    output_record = []\n",
        "    if name:\n",
        "        input_record.append(name)\n",
        "        output_record.append(name)\n",
        "    if date_1:\n",
        "        input_record.append(f\"Date: {date_1}\")\n",
        "        output_record.append(f\"Date: {date_1}\")\n",
        "    if dob:\n",
        "        input_record.append(f\"DOB: {dob}\")\n",
        "        output_record.append(f\"DOB: {dob}\")\n",
        "    if body_part and scan_type:\n",
        "        input_record.append(f\"{body_part} {scan_type} Scan\")\n",
        "\n",
        "    return \"\\n\".join(input_record), \"\\n\".join(output_record)\n",
        "\n",
        "input_records = []\n",
        "output_records = []\n",
        "# Generate 1000 records\n",
        "for _ in range(5000):\n",
        "    input_record, output_record = generate_random_record()\n",
        "    input_records.append(input_record)\n",
        "    output_records.append(output_record)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "txt_df = pd.DataFrame({\"instruction\": [\"Extract entities from the given text\"]*5000, \"input\": input_records, \"output\": output_records})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wPZPwZE16ToH",
        "outputId": "a0b62bc3-d5c4-4e2a-ec31-64be4dbc8012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            instruction  \\\n",
              "0  Extract entities from the given text   \n",
              "1  Extract entities from the given text   \n",
              "2  Extract entities from the given text   \n",
              "3  Extract entities from the given text   \n",
              "4  Extract entities from the given text   \n",
              "\n",
              "                                         input  \\\n",
              "0  Jacob Sm(th\\nDOB: 09.24.1947\\nElboU M% Scan   \n",
              "1            Date: 12.02@1997\\nDOB: 12.A5.1991   \n",
              "2               Cody Andervon\\nDOB: 06.26.19\"4   \n",
              "3                 J1hn Perez\\nDate: 06.28.?007   \n",
              "4              Re:inald Woods\\nDOB: 12.18. 993   \n",
              "\n",
              "                              output  \n",
              "0       Jacob Sm(th\\nDOB: 09.24.1947  \n",
              "1  Date: 12.02@1997\\nDOB: 12.A5.1991  \n",
              "2     Cody Andervon\\nDOB: 06.26.19\"4  \n",
              "3       J1hn Perez\\nDate: 06.28.?007  \n",
              "4    Re:inald Woods\\nDOB: 12.18. 993  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79e4e2e8-f8ab-492d-8335-5cf505570eb9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Jacob Sm(th\\nDOB: 09.24.1947\\nElboU M% Scan</td>\n",
              "      <td>Jacob Sm(th\\nDOB: 09.24.1947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Date: 12.02@1997\\nDOB: 12.A5.1991</td>\n",
              "      <td>Date: 12.02@1997\\nDOB: 12.A5.1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Cody Andervon\\nDOB: 06.26.19\"4</td>\n",
              "      <td>Cody Andervon\\nDOB: 06.26.19\"4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>J1hn Perez\\nDate: 06.28.?007</td>\n",
              "      <td>J1hn Perez\\nDate: 06.28.?007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Re:inald Woods\\nDOB: 12.18. 993</td>\n",
              "      <td>Re:inald Woods\\nDOB: 12.18. 993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79e4e2e8-f8ab-492d-8335-5cf505570eb9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79e4e2e8-f8ab-492d-8335-5cf505570eb9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79e4e2e8-f8ab-492d-8335-5cf505570eb9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08da01e9-c601-4525-9e9a-d48b25df808b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08da01e9-c601-4525-9e9a-d48b25df808b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08da01e9-c601-4525-9e9a-d48b25df808b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "txt_df",
              "summary": "{\n  \"name\": \"txt_df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Extract entities from the given text\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4558,\n        \"samples\": [\n          \"Brian MuellNr\\nDate: 08.10.x017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4391,\n        \"samples\": [\n          \"MWry Lee\\nDOB: 06.Q3.2004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "#!autotrain setup --colab > setup_logs.txt\n",
        "from autotrain import __version__\n",
        "print(f'AutoTrain version: {__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_j8uEmf9zxC",
        "outputId": "ec55fa80-fb20-4f2b-da68-504ca7bb1011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoTrain version: 0.7.129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n",
        "\n",
        "text_col = []\n",
        "for _, row in txt_df.iterrows():\n",
        "    instruction = row['instruction']\n",
        "    input_query = row['input']\n",
        "    response = row['output']\n",
        "\n",
        "    text = prompt + \"### Instruction:\\n\" + instruction + \"\\n### Input:\\n\" + input_query + \"\\n### Response:\\n\" + response\n",
        "    text_col.append(text)\n",
        "\n",
        "txt_df['text'] = text_col\n",
        "txt_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2XRu7oMg-4-f",
        "outputId": "9f4c61ca-a389-4cdf-9afa-57c2e87d2fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            instruction  \\\n",
              "0  Extract entities from the given text   \n",
              "1  Extract entities from the given text   \n",
              "2  Extract entities from the given text   \n",
              "3  Extract entities from the given text   \n",
              "4  Extract entities from the given text   \n",
              "\n",
              "                                         input  \\\n",
              "0  Jacob Sm(th\\nDOB: 09.24.1947\\nElboU M% Scan   \n",
              "1            Date: 12.02@1997\\nDOB: 12.A5.1991   \n",
              "2               Cody Andervon\\nDOB: 06.26.19\"4   \n",
              "3                 J1hn Perez\\nDate: 06.28.?007   \n",
              "4              Re:inald Woods\\nDOB: 12.18. 993   \n",
              "\n",
              "                              output  \\\n",
              "0       Jacob Sm(th\\nDOB: 09.24.1947   \n",
              "1  Date: 12.02@1997\\nDOB: 12.A5.1991   \n",
              "2     Cody Andervon\\nDOB: 06.26.19\"4   \n",
              "3       J1hn Perez\\nDate: 06.28.?007   \n",
              "4    Re:inald Woods\\nDOB: 12.18. 993   \n",
              "\n",
              "                                                text  \n",
              "0  Below is an instruction that describes a task,...  \n",
              "1  Below is an instruction that describes a task,...  \n",
              "2  Below is an instruction that describes a task,...  \n",
              "3  Below is an instruction that describes a task,...  \n",
              "4  Below is an instruction that describes a task,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07681a7a-2ff7-44cd-82bd-98d790619d0b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Jacob Sm(th\\nDOB: 09.24.1947\\nElboU M% Scan</td>\n",
              "      <td>Jacob Sm(th\\nDOB: 09.24.1947</td>\n",
              "      <td>Below is an instruction that describes a task,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Date: 12.02@1997\\nDOB: 12.A5.1991</td>\n",
              "      <td>Date: 12.02@1997\\nDOB: 12.A5.1991</td>\n",
              "      <td>Below is an instruction that describes a task,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Cody Andervon\\nDOB: 06.26.19\"4</td>\n",
              "      <td>Cody Andervon\\nDOB: 06.26.19\"4</td>\n",
              "      <td>Below is an instruction that describes a task,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>J1hn Perez\\nDate: 06.28.?007</td>\n",
              "      <td>J1hn Perez\\nDate: 06.28.?007</td>\n",
              "      <td>Below is an instruction that describes a task,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Extract entities from the given text</td>\n",
              "      <td>Re:inald Woods\\nDOB: 12.18. 993</td>\n",
              "      <td>Re:inald Woods\\nDOB: 12.18. 993</td>\n",
              "      <td>Below is an instruction that describes a task,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07681a7a-2ff7-44cd-82bd-98d790619d0b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-07681a7a-2ff7-44cd-82bd-98d790619d0b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-07681a7a-2ff7-44cd-82bd-98d790619d0b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f7dd70e9-c4ce-4f38-92e2-585f291f75ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f7dd70e9-c4ce-4f38-92e2-585f291f75ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f7dd70e9-c4ce-4f38-92e2-585f291f75ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "txt_df",
              "summary": "{\n  \"name\": \"txt_df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Extract entities from the given text\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4558,\n        \"samples\": [\n          \"Brian MuellNr\\nDate: 08.10.x017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4391,\n        \"samples\": [\n          \"MWry Lee\\nDOB: 06.Q3.2004\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4558,\n        \"samples\": [\n          \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExtract entities from the given text\\n### Input:\\nBrian MuellNr\\nDate: 08.10.x017\\n### Response:\\nBrian MuellNr\\nDate: 08.10.x017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_df.to_csv('train.csv', index=False)"
      ],
      "metadata": {
        "id": "k3iSBvCsDOnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = 'dicomllm'\n",
        "model_name = 'abhishek/llama-2-7b-hf-small-shards'\n",
        "\n",
        "push_to_hub = True\n",
        "hf_token = \"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\"\n",
        "hf_username = \"outlier9322\"\n",
        "\n",
        "unsloth = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 30 #@param {type:\"number\"}\n",
        "batch_size = 5 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 1024 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"generic\", \"sft\"] {type:\"string\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"string\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"string\"}\n",
        "lora_r = 16 #@param {type:\"number\"}\n",
        "lora_alpha = 32 #@param {type:\"number\"}\n",
        "lora_dropout = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"HF_USERNAME\"] = hf_username\n",
        "\n",
        "conf = f\"\"\"\n",
        "task: llm-{trainer}\n",
        "base_model: {model_name}\n",
        "project_name: {project_name}\n",
        "log: tensorboard\n",
        "backend: local\n",
        "\n",
        "data:\n",
        "  path: data\n",
        "  train_split: train\n",
        "  valid_split: null\n",
        "  chat_template: null\n",
        "  column_mapping:\n",
        "    text_column: text\n",
        "\n",
        "params:\n",
        "  block_size: {block_size}\n",
        "  lr: {learning_rate}\n",
        "  warmup_ratio: {warmup_ratio}\n",
        "  weight_decay: {weight_decay}\n",
        "  epochs: {num_epochs}\n",
        "  batch_size: {batch_size}\n",
        "  gradient_accumulation: {gradient_accumulation}\n",
        "  mixed_precision: {mixed_precision}\n",
        "  peft: {peft}\n",
        "  quantization: {quantization}\n",
        "  lora_r: {lora_r}\n",
        "  lora_alpha: {lora_alpha}\n",
        "  lora_dropout: {lora_dropout}\n",
        "  unsloth: {unsloth}\n",
        "\n",
        "hub:\n",
        "  username: ${{HF_USERNAME}}\n",
        "  token: ${{HF_TOKEN}}\n",
        "  push_to_hub: {push_to_hub}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"conf.yaml\", \"w\") as f:\n",
        "  f.write(conf)"
      ],
      "metadata": {
        "id": "hVgqZdQpFp8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain --config conf.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DagfUf6jIWXo",
        "outputId": "0db132a2-ccd3-48b4-e18e-3f737e776463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.cli.autotrain\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mUsing AutoTrain configuration: conf.yaml\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mRunning task: lm_training\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mUsing backend: local\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1m{'model': 'abhishek/llama-2-7b-hf-small-shards', 'project_name': 'dicomllm', 'data_path': 'data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 30, 'batch_size': 5, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': None, 'text_column': 'text', 'rejected_text_column': None, 'push_to_hub': True, 'username': 'outlier9322', 'token': '*****', 'unsloth': False}\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'dicomllm/training_params.json']\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:20\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'abhishek/llama-2-7b-hf-small-shards', 'project_name': 'dicomllm', 'data_path': 'data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'tensorboard', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 30, 'batch_size': 5, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': None, 'text_column': 'text', 'rejected_text_column': None, 'push_to_hub': True, 'username': 'outlier9322', 'token': '*****', 'unsloth': False}\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:30\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
            "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-06-24 07:53:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 28, in train\n",
            "    train_sft(config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/train_clm_sft.py\", line 15, in train\n",
            "    train_data, valid_data = utils.process_input_data(config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/utils.py\", line 347, in process_input_data\n",
            "    train_data = load_dataset(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2587, in load_dataset\n",
            "    builder_instance = load_dataset_builder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2259, in load_dataset_builder\n",
            "    dataset_module = dataset_module_factory(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 1904, in dataset_module_factory\n",
            "    raise e1 from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 1846, in dataset_module_factory\n",
            "    raise DatasetNotFoundError(msg + f\" at revision '{revision}'\" if revision else msg)\n",
            "datasets.exceptions.DatasetNotFoundError: Dataset 'data' doesn't exist on the Hub or cannot be accessed\n",
            "\u001b[0m\n",
            "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-06-24 07:53:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mDataset 'data' doesn't exist on the Hub or cannot be accessed\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 07:53:32\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[1mJob ID: 5507\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "NaGhz0KZI6TY",
        "outputId": "76c14556-9cc3-4de2-8b2e-490170923d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-132b64be65c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOiCEfnmJLnx",
        "outputId": "fd2371d7-5e32-4e9b-9689-de5694f47700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain setup --update-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0AlAouhS6Fe",
        "outputId": "501a4049-9bf1-43e6-f7a1-40a27ff89fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:00\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mInstalling latest xformers\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:01\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSuccessfully installed latest xformers\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:01\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mInstalling latest PyTorch\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:10\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mSuccessfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain llm --train --project-name dicomllm --model abhishek/llama-2-7b-hf-small-shards --data-path . --use-peft --quantization int4 --lr 2e-4 --train-batch-size 12 --epochs 3 --trainer sft --push-to-hub --username outlier9322 --token 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDAD4yrhJ_wz",
        "outputId": "614d8667-8be3-479e-8b9e-05c3a537af35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:43\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
            "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-06-24 09:24:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: version, train, config, func, inference, deploy, backend\u001b[0m\n",
            "\rSaving the dataset (0/1 shards):   0% 0/5000 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 5000/5000 [00:00<00:00, 699843.82 examples/s]\rSaving the dataset (1/1 shards): 100% 5000/5000 [00:00<00:00, 684963.26 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/5000 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 5000/5000 [00:00<00:00, 756002.88 examples/s]\rSaving the dataset (1/1 shards): 100% 5000/5000 [00:00<00:00, 742328.41 examples/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:43\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:43\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'dicomllm/training_params.json']\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:43\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'abhishek/llama-2-7b-hf-small-shards', 'project_name': 'dicomllm', 'data_path': 'dicomllm/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 3, 'batch_size': 12, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': True, 'username': 'outlier9322', 'token': '*****', 'unsloth': False}\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:53\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:53\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:53\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'autotrain_text'],\n",
            "    num_rows: 5000\n",
            "})\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:53\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "tokenizer_config.json: 100% 746/746 [00:00<00:00, 5.83MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 83.2MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 4.16MB/s]\n",
            "added_tokens.json: 100% 21.0/21.0 [00:00<00:00, 177kB/s]\n",
            "special_tokens_map.json: 100% 435/435 [00:00<00:00, 3.52MB/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:56\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:56\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:56\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:56\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 608/608 [00:00<00:00, 5.12MB/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:57\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
            "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-06-24 09:24:57\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:57\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:24:57\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 106MB/s]\n",
            "Downloading shards:   0% 0/10 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00010.bin:   0% 0.00/2.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   1% 31.5M/2.95G [00:00<00:10, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   3% 83.9M/2.95G [00:00<00:09, 293MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   4% 115M/2.95G [00:00<00:10, 280MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   6% 168M/2.95G [00:00<00:08, 339MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   7% 210M/2.95G [00:00<00:08, 339MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:   9% 252M/2.95G [00:00<00:10, 261MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  10% 304M/2.95G [00:00<00:08, 312MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  12% 357M/2.95G [00:01<00:07, 356MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  14% 409M/2.95G [00:01<00:06, 390MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  16% 461M/2.95G [00:01<00:05, 416MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  17% 514M/2.95G [00:01<00:05, 437MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  19% 566M/2.95G [00:01<00:05, 442MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  21% 619M/2.95G [00:01<00:05, 443MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  23% 671M/2.95G [00:01<00:05, 441MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  25% 724M/2.95G [00:01<00:05, 440MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  26% 776M/2.95G [00:02<00:04, 436MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  28% 828M/2.95G [00:02<00:05, 422MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  30% 881M/2.95G [00:02<00:04, 419MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  32% 933M/2.95G [00:02<00:05, 379MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  33% 986M/2.95G [00:02<00:05, 392MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  35% 1.04G/2.95G [00:02<00:04, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  37% 1.09G/2.95G [00:02<00:04, 431MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  39% 1.14G/2.95G [00:02<00:04, 423MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  40% 1.20G/2.95G [00:03<00:04, 436MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  42% 1.25G/2.95G [00:03<00:03, 442MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  44% 1.30G/2.95G [00:03<00:03, 444MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  46% 1.35G/2.95G [00:03<00:03, 445MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  48% 1.41G/2.95G [00:03<00:04, 384MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  49% 1.45G/2.95G [00:03<00:04, 348MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  51% 1.50G/2.95G [00:03<00:03, 374MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  53% 1.55G/2.95G [00:03<00:03, 397MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  54% 1.60G/2.95G [00:04<00:03, 414MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  56% 1.66G/2.95G [00:04<00:03, 426MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  58% 1.71G/2.95G [00:04<00:02, 440MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  60% 1.76G/2.95G [00:04<00:02, 442MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  61% 1.81G/2.95G [00:04<00:02, 451MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  63% 1.87G/2.95G [00:04<00:02, 454MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  65% 1.92G/2.95G [00:04<00:02, 451MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  67% 1.97G/2.95G [00:04<00:02, 451MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  69% 2.02G/2.95G [00:04<00:02, 452MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  70% 2.08G/2.95G [00:05<00:01, 456MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  72% 2.13G/2.95G [00:05<00:01, 454MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  74% 2.18G/2.95G [00:05<00:01, 455MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  76% 2.23G/2.95G [00:05<00:01, 455MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  77% 2.29G/2.95G [00:05<00:01, 453MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  79% 2.34G/2.95G [00:05<00:01, 458MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  81% 2.39G/2.95G [00:05<00:01, 454MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  83% 2.44G/2.95G [00:05<00:01, 459MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  85% 2.50G/2.95G [00:06<00:00, 459MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  86% 2.55G/2.95G [00:06<00:00, 460MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  88% 2.60G/2.95G [00:06<00:00, 465MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  90% 2.65G/2.95G [00:06<00:00, 468MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  92% 2.71G/2.95G [00:06<00:00, 468MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  93% 2.76G/2.95G [00:06<00:00, 398MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  95% 2.81G/2.95G [00:06<00:00, 419MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin:  97% 2.86G/2.95G [00:06<00:00, 430MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00010.bin: 100% 2.95G/2.95G [00:07<00:00, 418MB/s]\n",
            "Downloading shards:  10% 1/10 [00:07<01:05,  7.32s/it]\n",
            "pytorch_model-00002-of-00010.bin:   0% 0.00/2.88G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:   1% 41.9M/2.88G [00:00<00:07, 379MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:   3% 94.4M/2.88G [00:00<00:06, 461MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:   5% 147M/2.88G [00:00<00:05, 484MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00010.bin:   7% 210M/2.88G [00:00<00:05, 502MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:   9% 273M/2.88G [00:00<00:05, 512MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  11% 325M/2.88G [00:00<00:05, 478MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  13% 388M/2.88G [00:00<00:05, 497MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  15% 440M/2.88G [00:00<00:05, 478MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  17% 493M/2.88G [00:01<00:05, 429MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  19% 545M/2.88G [00:01<00:05, 432MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  21% 598M/2.88G [00:01<00:05, 416MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  23% 650M/2.88G [00:01<00:05, 424MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  24% 703M/2.88G [00:01<00:05, 397MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  26% 744M/2.88G [00:01<00:06, 347MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  28% 797M/2.88G [00:01<00:05, 384MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  30% 849M/2.88G [00:01<00:05, 383MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  31% 891M/2.88G [00:02<00:06, 313MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  32% 933M/2.88G [00:02<00:06, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  34% 965M/2.88G [00:02<00:07, 260MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  35% 996M/2.88G [00:02<00:07, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  36% 1.03G/2.88G [00:02<00:07, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  37% 1.06G/2.88G [00:02<00:07, 232MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  38% 1.09G/2.88G [00:03<00:07, 234MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  39% 1.12G/2.88G [00:03<00:10, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  40% 1.16G/2.88G [00:03<00:09, 184MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  41% 1.18G/2.88G [00:03<00:13, 126MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  43% 1.25G/2.88G [00:04<00:08, 196MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  45% 1.30G/2.88G [00:04<00:06, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  47% 1.34G/2.88G [00:04<00:05, 268MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  48% 1.39G/2.88G [00:04<00:04, 312MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  50% 1.45G/2.88G [00:04<00:04, 356MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  52% 1.50G/2.88G [00:04<00:03, 391MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  54% 1.55G/2.88G [00:04<00:03, 410MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  56% 1.60G/2.88G [00:04<00:02, 433MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  58% 1.66G/2.88G [00:04<00:02, 453MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  59% 1.71G/2.88G [00:05<00:02, 467MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  61% 1.76G/2.88G [00:05<00:02, 478MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  63% 1.81G/2.88G [00:05<00:02, 483MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  65% 1.87G/2.88G [00:05<00:02, 490MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  67% 1.92G/2.88G [00:05<00:01, 486MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  69% 1.97G/2.88G [00:05<00:01, 483MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  70% 2.02G/2.88G [00:05<00:01, 482MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  72% 2.08G/2.88G [00:05<00:01, 486MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  74% 2.13G/2.88G [00:05<00:01, 492MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  76% 2.18G/2.88G [00:06<00:01, 498MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  78% 2.23G/2.88G [00:06<00:01, 501MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  79% 2.29G/2.88G [00:06<00:01, 403MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  81% 2.34G/2.88G [00:06<00:02, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  83% 2.39G/2.88G [00:06<00:01, 275MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  85% 2.43G/2.88G [00:07<00:01, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  86% 2.49G/2.88G [00:07<00:01, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  88% 2.54G/2.88G [00:07<00:01, 332MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  90% 2.59G/2.88G [00:07<00:00, 362MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  91% 2.63G/2.88G [00:12<00:07, 30.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  94% 2.69G/2.88G [00:12<00:03, 46.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  95% 2.75G/2.88G [00:12<00:02, 63.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin:  97% 2.80G/2.88G [00:12<00:00, 86.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00010.bin: 100% 2.88G/2.88G [00:12<00:00, 224MB/s]\n",
            "Downloading shards:  20% 2/10 [00:20<01:25, 10.71s/it]\n",
            "pytorch_model-00003-of-00010.bin:   0% 0.00/2.99G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   1% 41.9M/2.99G [00:00<00:07, 387MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   3% 83.9M/2.99G [00:00<00:09, 303MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   4% 126M/2.99G [00:00<00:08, 332MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   6% 168M/2.99G [00:00<00:09, 304MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   7% 220M/2.99G [00:00<00:07, 359MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:   9% 273M/2.99G [00:00<00:07, 385MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  11% 325M/2.99G [00:00<00:06, 408MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  12% 367M/2.99G [00:00<00:06, 407MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  14% 419M/2.99G [00:01<00:05, 432MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  16% 472M/2.99G [00:01<00:05, 445MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  18% 524M/2.99G [00:01<00:05, 450MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  20% 587M/2.99G [00:01<00:05, 479MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  21% 640M/2.99G [00:01<00:05, 437MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  23% 692M/2.99G [00:01<00:05, 446MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  25% 744M/2.99G [00:01<00:04, 450MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  27% 797M/2.99G [00:01<00:04, 462MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  28% 849M/2.99G [00:02<00:04, 464MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  30% 902M/2.99G [00:02<00:04, 463MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  32% 954M/2.99G [00:02<00:04, 451MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  34% 1.01G/2.99G [00:02<00:04, 450MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  35% 1.06G/2.99G [00:02<00:04, 449MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  37% 1.11G/2.99G [00:02<00:04, 406MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  39% 1.15G/2.99G [00:02<00:04, 403MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  40% 1.20G/2.99G [00:02<00:04, 401MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  42% 1.25G/2.99G [00:02<00:04, 414MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  43% 1.30G/2.99G [00:03<00:03, 443MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  45% 1.35G/2.99G [00:03<00:03, 458MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  47% 1.41G/2.99G [00:03<00:03, 469MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  49% 1.46G/2.99G [00:03<00:03, 456MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  50% 1.51G/2.99G [00:03<00:03, 427MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  52% 1.56G/2.99G [00:03<00:03, 426MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  54% 1.61G/2.99G [00:03<00:03, 435MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  56% 1.67G/2.99G [00:03<00:03, 392MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  58% 1.72G/2.99G [00:04<00:03, 412MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  59% 1.77G/2.99G [00:04<00:02, 426MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  61% 1.82G/2.99G [00:04<00:02, 440MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  63% 1.88G/2.99G [00:04<00:02, 455MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  65% 1.93G/2.99G [00:04<00:02, 464MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  66% 1.98G/2.99G [00:04<00:02, 462MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  68% 2.03G/2.99G [00:04<00:02, 472MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  70% 2.09G/2.99G [00:04<00:01, 484MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  72% 2.14G/2.99G [00:04<00:01, 489MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  73% 2.19G/2.99G [00:05<00:01, 494MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  75% 2.24G/2.99G [00:05<00:01, 497MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  77% 2.30G/2.99G [00:05<00:01, 489MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  79% 2.35G/2.99G [00:05<00:01, 474MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  80% 2.40G/2.99G [00:05<00:01, 471MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  82% 2.45G/2.99G [00:05<00:01, 457MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  84% 2.51G/2.99G [00:05<00:01, 424MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  86% 2.56G/2.99G [00:05<00:01, 407MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  87% 2.60G/2.99G [00:06<00:01, 355MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  88% 2.64G/2.99G [00:06<00:01, 342MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  90% 2.68G/2.99G [00:06<00:00, 314MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  91% 2.73G/2.99G [00:06<00:00, 329MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  93% 2.77G/2.99G [00:06<00:00, 335MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  94% 2.82G/2.99G [00:06<00:00, 350MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  96% 2.86G/2.99G [00:06<00:00, 339MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  97% 2.90G/2.99G [00:06<00:00, 336MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin:  99% 2.95G/2.99G [00:07<00:00, 337MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00010.bin: 100% 2.99G/2.99G [00:07<00:00, 415MB/s]\n",
            "Downloading shards:  30% 3/10 [00:27<01:04,  9.23s/it]\n",
            "pytorch_model-00004-of-00010.bin:   0% 0.00/2.86G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:   1% 41.9M/2.86G [00:00<00:07, 397MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:   3% 94.4M/2.86G [00:00<00:06, 445MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:   5% 147M/2.86G [00:00<00:05, 461MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00010.bin:   7% 199M/2.86G [00:00<00:05, 466MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:   9% 252M/2.86G [00:00<00:05, 475MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  11% 304M/2.86G [00:00<00:05, 450MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  12% 357M/2.86G [00:00<00:05, 450MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  14% 409M/2.86G [00:00<00:06, 393MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  16% 451M/2.86G [00:01<00:07, 312MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  17% 493M/2.86G [00:01<00:09, 258MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  18% 524M/2.86G [00:01<00:12, 191MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  19% 556M/2.86G [00:04<00:53, 43.1MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  21% 598M/2.86G [00:04<00:37, 60.4MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  23% 650M/2.86G [00:04<00:28, 78.4MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  24% 682M/2.86G [00:04<00:23, 94.3MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  25% 713M/2.86G [00:04<00:19, 110MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  26% 744M/2.86G [00:05<00:17, 119MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  28% 786M/2.86G [00:05<00:13, 154MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  29% 839M/2.86G [00:05<00:10, 197MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  30% 870M/2.86G [00:05<00:09, 216MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  32% 902M/2.86G [00:05<00:09, 213MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  33% 954M/2.86G [00:05<00:07, 265MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  35% 996M/2.86G [00:05<00:06, 286MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  36% 1.04G/2.86G [00:05<00:06, 299MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  38% 1.08G/2.86G [00:06<00:06, 281MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  39% 1.11G/2.86G [00:06<00:06, 263MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  40% 1.14G/2.86G [00:06<00:06, 247MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  41% 1.18G/2.86G [00:06<00:05, 279MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  43% 1.24G/2.86G [00:06<00:04, 333MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  45% 1.28G/2.86G [00:06<00:04, 354MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  47% 1.33G/2.86G [00:06<00:03, 386MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  48% 1.38G/2.86G [00:06<00:03, 394MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  50% 1.43G/2.86G [00:07<00:03, 400MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  51% 1.47G/2.86G [00:07<00:03, 360MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  53% 1.51G/2.86G [00:07<00:03, 353MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  54% 1.55G/2.86G [00:07<00:03, 353MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  56% 1.59G/2.86G [00:07<00:04, 298MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  57% 1.64G/2.86G [00:07<00:03, 309MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  59% 1.70G/2.86G [00:07<00:03, 365MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  61% 1.74G/2.86G [00:07<00:02, 377MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  62% 1.78G/2.86G [00:08<00:02, 381MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  65% 1.85G/2.86G [00:08<00:02, 426MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  67% 1.91G/2.86G [00:08<00:02, 458MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  69% 1.96G/2.86G [00:08<00:01, 461MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  70% 2.01G/2.86G [00:08<00:01, 445MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  72% 2.07G/2.86G [00:08<00:02, 350MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  74% 2.11G/2.86G [00:08<00:02, 353MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  75% 2.15G/2.86G [00:09<00:01, 357MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  77% 2.19G/2.86G [00:09<00:01, 364MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  79% 2.24G/2.86G [00:09<00:01, 398MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  80% 2.29G/2.86G [00:09<00:01, 394MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  82% 2.34G/2.86G [00:09<00:01, 423MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  84% 2.39G/2.86G [00:09<00:01, 428MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  86% 2.44G/2.86G [00:09<00:01, 354MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  87% 2.49G/2.86G [00:09<00:01, 322MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  89% 2.54G/2.86G [00:10<00:00, 366MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  90% 2.58G/2.86G [00:10<00:00, 357MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  92% 2.62G/2.86G [00:10<00:00, 359MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  94% 2.67G/2.86G [00:10<00:00, 383MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  95% 2.73G/2.86G [00:10<00:00, 400MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin:  97% 2.78G/2.86G [00:10<00:00, 413MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00010.bin: 100% 2.86G/2.86G [00:10<00:00, 262MB/s]\n",
            "Downloading shards:  40% 4/10 [00:39<00:59,  9.98s/it]\n",
            "pytorch_model-00005-of-00010.bin:   0% 0.00/2.88G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:   1% 31.5M/2.88G [00:00<00:12, 224MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:   3% 73.4M/2.88G [00:00<00:08, 314MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:   4% 126M/2.88G [00:00<00:07, 374MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00010.bin:   6% 178M/2.88G [00:00<00:06, 397MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:   8% 231M/2.88G [00:00<00:06, 413MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  10% 283M/2.88G [00:00<00:06, 424MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  12% 336M/2.88G [00:00<00:05, 445MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  13% 388M/2.88G [00:00<00:05, 465MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  15% 440M/2.88G [00:01<00:06, 376MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  17% 482M/2.88G [00:01<00:06, 361MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  18% 524M/2.88G [00:01<00:06, 368MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  20% 566M/2.88G [00:01<00:06, 377MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  21% 608M/2.88G [00:01<00:05, 382MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  23% 650M/2.88G [00:01<00:06, 360MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  24% 692M/2.88G [00:01<00:06, 356MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  26% 734M/2.88G [00:01<00:05, 368MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  27% 776M/2.88G [00:02<00:06, 324MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  28% 818M/2.88G [00:02<00:07, 283MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  30% 870M/2.88G [00:02<00:06, 322MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  32% 923M/2.88G [00:02<00:05, 349MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  34% 975M/2.88G [00:02<00:05, 380MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  36% 1.03G/2.88G [00:02<00:04, 395MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  38% 1.08G/2.88G [00:02<00:04, 371MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  39% 1.12G/2.88G [00:03<00:04, 371MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  40% 1.16G/2.88G [00:03<00:04, 368MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  42% 1.21G/2.88G [00:03<00:04, 377MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  44% 1.26G/2.88G [00:03<00:04, 384MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  46% 1.31G/2.88G [00:03<00:03, 395MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  47% 1.35G/2.88G [00:03<00:03, 388MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  49% 1.41G/2.88G [00:03<00:03, 408MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  51% 1.46G/2.88G [00:03<00:03, 423MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  52% 1.51G/2.88G [00:03<00:03, 445MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  54% 1.56G/2.88G [00:04<00:03, 423MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  56% 1.61G/2.88G [00:04<00:03, 400MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  58% 1.66G/2.88G [00:04<00:03, 388MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  59% 1.70G/2.88G [00:04<00:03, 371MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  60% 1.74G/2.88G [00:04<00:03, 357MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  62% 1.78G/2.88G [00:04<00:02, 368MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  63% 1.82G/2.88G [00:05<00:04, 258MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  65% 1.88G/2.88G [00:05<00:03, 304MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  67% 1.92G/2.88G [00:05<00:02, 322MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  68% 1.96G/2.88G [00:05<00:02, 310MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  70% 2.00G/2.88G [00:05<00:03, 291MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  71% 2.03G/2.88G [00:05<00:03, 276MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  72% 2.07G/2.88G [00:05<00:03, 265MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  73% 2.10G/2.88G [00:06<00:04, 172MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  74% 2.13G/2.88G [00:06<00:03, 192MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  75% 2.16G/2.88G [00:06<00:03, 197MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  76% 2.19G/2.88G [00:06<00:03, 203MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  77% 2.22G/2.88G [00:06<00:03, 211MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  78% 2.25G/2.88G [00:06<00:02, 209MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  79% 2.29G/2.88G [00:07<00:02, 212MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  81% 2.32G/2.88G [00:07<00:02, 211MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  82% 2.35G/2.88G [00:07<00:02, 214MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  83% 2.38G/2.88G [00:07<00:02, 226MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  84% 2.41G/2.88G [00:07<00:01, 234MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  85% 2.44G/2.88G [00:07<00:01, 243MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  86% 2.47G/2.88G [00:07<00:01, 249MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  87% 2.51G/2.88G [00:07<00:01, 257MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  88% 2.54G/2.88G [00:08<00:01, 247MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  89% 2.57G/2.88G [00:08<00:01, 238MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  90% 2.60G/2.88G [00:08<00:01, 228MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  91% 2.63G/2.88G [00:08<00:01, 231MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  93% 2.66G/2.88G [00:08<00:00, 219MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  94% 2.69G/2.88G [00:08<00:01, 162MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  94% 2.72G/2.88G [00:10<00:03, 43.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  95% 2.74G/2.88G [00:13<00:07, 19.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  96% 2.76G/2.88G [00:13<00:04, 24.5MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  97% 2.78G/2.88G [00:13<00:03, 32.1MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin:  98% 2.83G/2.88G [00:14<00:00, 58.9MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00010.bin: 100% 2.88G/2.88G [00:14<00:00, 203MB/s] \n",
            "Downloading shards:  50% 5/10 [00:53<00:57, 11.59s/it]\n",
            "pytorch_model-00006-of-00010.bin:   0% 0.00/2.97G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:   1% 41.9M/2.97G [00:00<00:08, 360MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:   3% 94.4M/2.97G [00:00<00:06, 439MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:   5% 147M/2.97G [00:00<00:06, 431MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00010.bin:   7% 199M/2.97G [00:00<00:06, 450MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:   8% 252M/2.97G [00:00<00:05, 464MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  10% 304M/2.97G [00:00<00:05, 476MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  12% 357M/2.97G [00:00<00:05, 477MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  14% 409M/2.97G [00:00<00:05, 473MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  16% 461M/2.97G [00:01<00:06, 411MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  17% 514M/2.97G [00:01<00:07, 320MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  19% 556M/2.97G [00:01<00:07, 323MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  20% 598M/2.97G [00:01<00:06, 343MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  22% 650M/2.97G [00:01<00:06, 374MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  23% 692M/2.97G [00:01<00:06, 379MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  25% 734M/2.97G [00:01<00:06, 359MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  26% 776M/2.97G [00:02<00:06, 346MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  28% 828M/2.97G [00:02<00:05, 366MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  29% 870M/2.97G [00:02<00:05, 376MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  31% 923M/2.97G [00:02<00:05, 404MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  33% 986M/2.97G [00:02<00:04, 447MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  35% 1.04G/2.97G [00:02<00:04, 428MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  37% 1.09G/2.97G [00:02<00:05, 346MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  38% 1.14G/2.97G [00:02<00:04, 372MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  40% 1.20G/2.97G [00:03<00:04, 386MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  42% 1.24G/2.97G [00:03<00:04, 386MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  43% 1.29G/2.97G [00:03<00:04, 406MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  45% 1.34G/2.97G [00:03<00:03, 418MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  47% 1.39G/2.97G [00:03<00:03, 426MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  49% 1.45G/2.97G [00:03<00:03, 426MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  50% 1.50G/2.97G [00:03<00:03, 436MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  52% 1.55G/2.97G [00:03<00:03, 441MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  54% 1.60G/2.97G [00:03<00:03, 447MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  56% 1.66G/2.97G [00:04<00:02, 450MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  58% 1.71G/2.97G [00:04<00:02, 449MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  59% 1.76G/2.97G [00:04<00:02, 449MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  61% 1.81G/2.97G [00:04<00:02, 450MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  63% 1.87G/2.97G [00:04<00:02, 420MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  65% 1.92G/2.97G [00:04<00:02, 416MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  66% 1.96G/2.97G [00:04<00:02, 412MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  68% 2.01G/2.97G [00:04<00:02, 419MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  70% 2.07G/2.97G [00:05<00:02, 422MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  71% 2.12G/2.97G [00:05<00:01, 427MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  73% 2.17G/2.97G [00:05<00:01, 428MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  75% 2.22G/2.97G [00:05<00:01, 422MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  77% 2.28G/2.97G [00:05<00:01, 414MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  78% 2.32G/2.97G [00:05<00:01, 404MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  79% 2.36G/2.97G [00:05<00:01, 398MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  81% 2.40G/2.97G [00:05<00:01, 396MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  82% 2.44G/2.97G [00:06<00:01, 389MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  84% 2.49G/2.97G [00:06<00:01, 388MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  85% 2.53G/2.97G [00:06<00:01, 396MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  87% 2.57G/2.97G [00:06<00:01, 399MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  88% 2.61G/2.97G [00:06<00:00, 398MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  89% 2.65G/2.97G [00:06<00:00, 392MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  91% 2.69G/2.97G [00:06<00:00, 386MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  92% 2.74G/2.97G [00:06<00:00, 384MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  94% 2.78G/2.97G [00:06<00:00, 386MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  95% 2.82G/2.97G [00:06<00:00, 389MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  97% 2.87G/2.97G [00:07<00:00, 412MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin:  99% 2.93G/2.97G [00:07<00:00, 428MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00010.bin: 100% 2.97G/2.97G [00:07<00:00, 406MB/s]\n",
            "Downloading shards:  60% 6/10 [01:01<00:40, 10.22s/it]\n",
            "pytorch_model-00007-of-00010.bin:   0% 0.00/2.88G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:   2% 52.4M/2.88G [00:00<00:06, 449MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:   4% 105M/2.88G [00:00<00:06, 450MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00010.bin:   5% 157M/2.88G [00:00<00:06, 438MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:   7% 210M/2.88G [00:00<00:06, 430MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:   9% 262M/2.88G [00:00<00:06, 418MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  11% 304M/2.88G [00:00<00:06, 410MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  12% 346M/2.88G [00:00<00:06, 404MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  13% 388M/2.88G [00:00<00:06, 376MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  15% 440M/2.88G [00:01<00:06, 402MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  17% 493M/2.88G [00:01<00:05, 417MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  19% 545M/2.88G [00:01<00:05, 429MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  21% 598M/2.88G [00:01<00:05, 434MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  23% 650M/2.88G [00:01<00:05, 440MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  24% 703M/2.88G [00:01<00:04, 446MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  26% 755M/2.88G [00:01<00:04, 442MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  28% 807M/2.88G [00:02<00:07, 295MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  30% 849M/2.88G [00:02<00:06, 314MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  31% 891M/2.88G [00:02<00:06, 331MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  33% 944M/2.88G [00:02<00:05, 356MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  35% 996M/2.88G [00:02<00:04, 379MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  36% 1.05G/2.88G [00:02<00:04, 392MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  38% 1.10G/2.88G [00:02<00:04, 404MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  40% 1.15G/2.88G [00:02<00:04, 425MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  42% 1.21G/2.88G [00:03<00:03, 424MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  44% 1.26G/2.88G [00:03<00:03, 427MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  46% 1.31G/2.88G [00:03<00:03, 427MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  47% 1.36G/2.88G [00:03<00:03, 427MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  49% 1.42G/2.88G [00:03<00:03, 428MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  51% 1.47G/2.88G [00:03<00:03, 423MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  53% 1.52G/2.88G [00:03<00:03, 380MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  54% 1.56G/2.88G [00:03<00:03, 375MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  56% 1.60G/2.88G [00:04<00:03, 377MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  58% 1.66G/2.88G [00:04<00:03, 382MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  59% 1.70G/2.88G [00:04<00:03, 362MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  60% 1.74G/2.88G [00:04<00:03, 364MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  62% 1.78G/2.88G [00:04<00:02, 375MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  63% 1.82G/2.88G [00:04<00:02, 386MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  65% 1.88G/2.88G [00:04<00:02, 400MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  67% 1.93G/2.88G [00:04<00:02, 409MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  69% 1.97G/2.88G [00:04<00:02, 411MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  70% 2.01G/2.88G [00:05<00:02, 413MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  72% 2.07G/2.88G [00:05<00:01, 432MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  74% 2.12G/2.88G [00:05<00:01, 424MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  75% 2.17G/2.88G [00:05<00:01, 369MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  77% 2.21G/2.88G [00:05<00:01, 363MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  78% 2.25G/2.88G [00:05<00:01, 364MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  80% 2.31G/2.88G [00:05<00:01, 383MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  82% 2.35G/2.88G [00:05<00:01, 366MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  83% 2.39G/2.88G [00:06<00:01, 375MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  85% 2.43G/2.88G [00:06<00:01, 381MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  86% 2.47G/2.88G [00:06<00:01, 379MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  87% 2.52G/2.88G [00:06<00:00, 363MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  89% 2.56G/2.88G [00:06<00:00, 365MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  91% 2.61G/2.88G [00:06<00:00, 396MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  92% 2.65G/2.88G [00:06<00:00, 399MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  94% 2.71G/2.88G [00:06<00:00, 406MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  95% 2.75G/2.88G [00:06<00:00, 404MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  97% 2.79G/2.88G [00:07<00:00, 389MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin:  98% 2.83G/2.88G [00:07<00:00, 384MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00010.bin: 100% 2.88G/2.88G [00:07<00:00, 391MB/s]\n",
            "Downloading shards:  70% 7/10 [01:08<00:28,  9.37s/it]\n",
            "pytorch_model-00008-of-00010.bin:   0% 0.00/2.99G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   1% 41.9M/2.99G [00:00<00:07, 372MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   3% 83.9M/2.99G [00:00<00:07, 386MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   4% 126M/2.99G [00:00<00:07, 368MB/s] \u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   6% 168M/2.99G [00:00<00:07, 364MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   7% 220M/2.99G [00:00<00:07, 395MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:   9% 262M/2.99G [00:00<00:07, 365MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  11% 315M/2.99G [00:00<00:06, 392MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  12% 367M/2.99G [00:00<00:06, 414MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  14% 419M/2.99G [00:01<00:05, 429MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  16% 472M/2.99G [00:01<00:05, 436MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  18% 524M/2.99G [00:01<00:05, 439MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  19% 577M/2.99G [00:01<00:06, 388MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  21% 619M/2.99G [00:01<00:06, 358MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  22% 661M/2.99G [00:01<00:06, 361MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  23% 703M/2.99G [00:01<00:06, 372MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  25% 755M/2.99G [00:01<00:05, 400MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  27% 807M/2.99G [00:02<00:05, 433MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  29% 860M/2.99G [00:02<00:04, 437MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  31% 912M/2.99G [00:02<00:04, 417MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  32% 965M/2.99G [00:02<00:04, 426MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  34% 1.02G/2.99G [00:02<00:04, 425MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  36% 1.07G/2.99G [00:02<00:04, 426MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  38% 1.12G/2.99G [00:02<00:04, 394MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  39% 1.17G/2.99G [00:02<00:04, 413MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  41% 1.23G/2.99G [00:03<00:04, 399MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  42% 1.27G/2.99G [00:03<00:04, 363MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  44% 1.31G/2.99G [00:03<00:04, 355MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  45% 1.35G/2.99G [00:03<00:04, 353MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  47% 1.39G/2.99G [00:03<00:04, 343MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  48% 1.45G/2.99G [00:03<00:04, 375MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  50% 1.50G/2.99G [00:03<00:03, 403MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  52% 1.54G/2.99G [00:03<00:03, 383MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  53% 1.58G/2.99G [00:04<00:03, 383MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  54% 1.63G/2.99G [00:04<00:04, 334MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  56% 1.67G/2.99G [00:04<00:04, 295MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  57% 1.70G/2.99G [00:04<00:06, 208MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  59% 1.75G/2.99G [00:04<00:04, 252MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  60% 1.78G/2.99G [00:05<00:06, 194MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  61% 1.81G/2.99G [00:05<00:06, 181MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  62% 1.85G/2.99G [00:05<00:06, 173MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  62% 1.87G/2.99G [00:05<00:06, 164MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  63% 1.89G/2.99G [00:05<00:07, 145MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  64% 1.91G/2.99G [00:06<00:09, 111MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  65% 1.93G/2.99G [00:11<01:20, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  65% 1.94G/2.99G [00:12<01:13, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  65% 1.95G/2.99G [00:12<01:07, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  66% 1.96G/2.99G [00:13<01:03, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  66% 1.97G/2.99G [00:14<01:15, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  66% 1.98G/2.99G [00:18<02:25, 6.94MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  67% 1.99G/2.99G [00:24<04:19, 3.85MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  67% 2.00G/2.99G [00:30<05:48, 2.84MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  67% 2.01G/2.99G [00:35<06:15, 2.61MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  68% 2.02G/2.99G [00:43<07:55, 2.03MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  68% 2.03G/2.99G [00:45<06:22, 2.50MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  68% 2.04G/2.99G [00:46<04:57, 3.18MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  69% 2.06G/2.99G [00:47<03:47, 4.12MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  69% 2.07G/2.99G [00:47<02:41, 5.72MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  70% 2.10G/2.99G [00:47<01:10, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  72% 2.14G/2.99G [00:47<00:33, 25.2MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  73% 2.18G/2.99G [00:47<00:19, 41.7MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  74% 2.22G/2.99G [00:47<00:12, 62.9MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  76% 2.28G/2.99G [00:47<00:07, 97.2MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  78% 2.33G/2.99G [00:48<00:04, 137MB/s] \u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  79% 2.37G/2.99G [00:48<00:03, 169MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  81% 2.42G/2.99G [00:48<00:02, 217MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  83% 2.47G/2.99G [00:48<00:01, 268MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  84% 2.53G/2.99G [00:48<00:01, 303MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  86% 2.58G/2.99G [00:48<00:01, 317MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  88% 2.63G/2.99G [00:48<00:01, 346MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  90% 2.68G/2.99G [00:48<00:00, 369MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  92% 2.74G/2.99G [00:49<00:00, 396MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  93% 2.79G/2.99G [00:49<00:00, 333MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  95% 2.83G/2.99G [00:49<00:00, 327MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  96% 2.87G/2.99G [00:49<00:00, 310MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  97% 2.92G/2.99G [00:49<00:00, 293MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin:  99% 2.96G/2.99G [00:49<00:00, 314MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00010.bin: 100% 2.99G/2.99G [00:49<00:00, 59.9MB/s]\n",
            "Downloading shards:  80% 8/10 [01:58<00:44, 22.35s/it]\n",
            "pytorch_model-00009-of-00010.bin:   0% 0.00/2.86G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   1% 31.5M/2.86G [00:00<00:09, 311MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   3% 73.4M/2.86G [00:00<00:08, 347MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   4% 115M/2.86G [00:00<00:11, 242MB/s] \u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   5% 147M/2.86G [00:00<00:10, 261MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   6% 178M/2.86G [00:00<00:09, 268MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   7% 210M/2.86G [00:00<00:10, 261MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:   9% 252M/2.86G [00:00<00:09, 274MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  10% 283M/2.86G [00:01<00:10, 234MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  11% 325M/2.86G [00:01<00:09, 264MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  12% 357M/2.86G [00:01<00:09, 262MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  14% 398M/2.86G [00:01<00:08, 275MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  15% 430M/2.86G [00:01<00:08, 279MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  17% 472M/2.86G [00:01<00:07, 307MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  18% 503M/2.86G [00:01<00:07, 306MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  19% 535M/2.86G [00:01<00:08, 279MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  20% 566M/2.86G [00:02<00:08, 260MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  21% 608M/2.86G [00:02<00:07, 294MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  22% 640M/2.86G [00:02<00:09, 232MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  23% 671M/2.86G [00:02<00:10, 201MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  25% 703M/2.86G [00:02<00:10, 208MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  26% 734M/2.86G [00:02<00:09, 218MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  27% 765M/2.86G [00:03<00:09, 213MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  28% 797M/2.86G [00:03<00:08, 229MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  29% 828M/2.86G [00:03<00:08, 232MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  30% 860M/2.86G [00:03<00:08, 247MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  31% 891M/2.86G [00:03<00:07, 264MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  33% 933M/2.86G [00:03<00:06, 285MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  34% 975M/2.86G [00:03<00:06, 306MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  35% 1.01G/2.86G [00:03<00:06, 304MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  37% 1.05G/2.86G [00:03<00:05, 310MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  38% 1.09G/2.86G [00:04<00:05, 339MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  40% 1.13G/2.86G [00:04<00:05, 333MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  41% 1.17G/2.86G [00:04<00:05, 298MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  42% 1.21G/2.86G [00:04<00:06, 272MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  44% 1.25G/2.86G [00:04<00:05, 299MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  45% 1.29G/2.86G [00:04<00:05, 294MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  47% 1.33G/2.86G [00:04<00:05, 305MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  48% 1.37G/2.86G [00:05<00:04, 330MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  50% 1.42G/2.86G [00:05<00:04, 333MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  51% 1.46G/2.86G [00:05<00:04, 319MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  52% 1.50G/2.86G [00:05<00:04, 314MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  54% 1.54G/2.86G [00:05<00:04, 284MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  55% 1.58G/2.86G [00:05<00:04, 309MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  57% 1.63G/2.86G [00:05<00:03, 318MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  58% 1.67G/2.86G [00:06<00:04, 287MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  59% 1.70G/2.86G [00:06<00:04, 285MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  61% 1.73G/2.86G [00:06<00:03, 286MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  62% 1.77G/2.86G [00:06<00:03, 308MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  63% 1.80G/2.86G [00:06<00:03, 304MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  64% 1.84G/2.86G [00:06<00:04, 218MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  65% 1.87G/2.86G [00:06<00:04, 217MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  67% 1.91G/2.86G [00:06<00:03, 250MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  68% 1.94G/2.86G [00:07<00:04, 204MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  69% 1.97G/2.86G [00:07<00:04, 206MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  70% 2.00G/2.86G [00:07<00:04, 205MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  72% 2.04G/2.86G [00:07<00:03, 243MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  73% 2.09G/2.86G [00:07<00:02, 265MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  75% 2.13G/2.86G [00:07<00:02, 284MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  76% 2.16G/2.86G [00:07<00:02, 286MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  77% 2.20G/2.86G [00:08<00:02, 306MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  79% 2.24G/2.86G [00:08<00:01, 321MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  80% 2.29G/2.86G [00:08<00:02, 261MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  81% 2.32G/2.86G [00:08<00:02, 248MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  82% 2.35G/2.86G [00:08<00:02, 218MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  83% 2.38G/2.86G [00:08<00:02, 237MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  84% 2.41G/2.86G [00:09<00:01, 238MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  86% 2.44G/2.86G [00:09<00:01, 223MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  87% 2.47G/2.86G [00:09<00:01, 231MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  88% 2.52G/2.86G [00:09<00:01, 260MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  89% 2.55G/2.86G [00:09<00:01, 262MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  90% 2.58G/2.86G [00:09<00:01, 259MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  91% 2.61G/2.86G [00:10<00:02, 97.7MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  93% 2.65G/2.86G [00:10<00:01, 132MB/s] \u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  95% 2.71G/2.86G [00:10<00:00, 179MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  96% 2.74G/2.86G [00:10<00:00, 193MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  97% 2.77G/2.86G [00:10<00:00, 205MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin:  98% 2.80G/2.86G [00:11<00:00, 214MB/s]\u001b[A\n",
            "pytorch_model-00009-of-00010.bin: 100% 2.86G/2.86G [00:11<00:00, 252MB/s]\n",
            "Downloading shards:  90% 9/10 [02:10<00:18, 18.99s/it]\n",
            "pytorch_model-00010-of-00010.bin:   0% 0.00/705M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:   6% 41.9M/705M [00:00<00:01, 388MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  12% 83.9M/705M [00:00<00:03, 192MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  19% 136M/705M [00:00<00:02, 269MB/s] \u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  25% 178M/705M [00:00<00:01, 275MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  30% 210M/705M [00:00<00:01, 278MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  34% 241M/705M [00:00<00:01, 272MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  40% 283M/705M [00:01<00:01, 307MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  46% 325M/705M [00:01<00:01, 338MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  52% 367M/705M [00:01<00:01, 291MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  58% 409M/705M [00:01<00:01, 258MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  62% 440M/705M [00:01<00:01, 238MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  67% 472M/705M [00:01<00:00, 255MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  71% 503M/705M [00:01<00:00, 228MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  77% 545M/705M [00:02<00:00, 248MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  83% 587M/705M [00:02<00:00, 267MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  88% 619M/705M [00:02<00:00, 250MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin:  92% 650M/705M [00:02<00:00, 236MB/s]\u001b[A\n",
            "pytorch_model-00010-of-00010.bin: 100% 705M/705M [00:02<00:00, 261MB/s]\n",
            "Downloading shards: 100% 10/10 [02:13<00:00, 13.33s/it]\n",
            "Loading checkpoint shards: 100% 10/10 [01:29<00:00,  8.97s/it]\n",
            "generation_config.json: 100% 174/174 [00:00<00:00, 1.14MB/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:28:44\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:28:44\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "Generating train split: 459 examples [00:01, 416.12 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:28:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
            " 21% 25/117 [26:27<1:37:23, 63.51s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 09:55:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8382, 'grad_norm': 0.20953379571437836, 'learning_rate': 0.00017523809523809525, 'epoch': 0.6410256410256411}\u001b[0m\n",
            "{'loss': 0.8382, 'grad_norm': 0.20953379571437836, 'learning_rate': 0.00017523809523809525, 'epoch': 0.64}\n",
            " 43% 50/117 [52:11<1:10:43, 63.34s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 10:20:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4204, 'grad_norm': 0.13946102559566498, 'learning_rate': 0.0001276190476190476, 'epoch': 1.282051282051282}\u001b[0m\n",
            "{'loss': 0.4204, 'grad_norm': 0.13946102559566498, 'learning_rate': 0.0001276190476190476, 'epoch': 1.28}\n",
            " 64% 75/117 [1:18:40<44:30, 63.58s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 10:47:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3915, 'grad_norm': 0.12348732352256775, 'learning_rate': 8e-05, 'epoch': 1.9230769230769231}\u001b[0m\n",
            "{'loss': 0.3915, 'grad_norm': 0.12348732352256775, 'learning_rate': 8e-05, 'epoch': 1.92}\n",
            " 85% 100/117 [1:44:23<18:00, 63.56s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 11:13:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3754, 'grad_norm': 0.13654549419879913, 'learning_rate': 3.2380952380952386e-05, 'epoch': 2.564102564102564}\u001b[0m\n",
            "{'loss': 0.3754, 'grad_norm': 0.13654549419879913, 'learning_rate': 3.2380952380952386e-05, 'epoch': 2.56}\n",
            "100% 117/117 [2:01:37<00:00, 49.62s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 11:30:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'train_runtime': 7297.6052, 'train_samples_per_second': 0.189, 'train_steps_per_second': 0.016, 'train_loss': 0.4867380337837415, 'epoch': 3.0}\u001b[0m\n",
            "{'train_runtime': 7297.6052, 'train_samples_per_second': 0.189, 'train_steps_per_second': 0.016, 'train_loss': 0.4867380337837415, 'epoch': 3.0}\n",
            "100% 117/117 [2:01:37<00:00, 62.37s/it]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 11:30:24\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mpost_training_steps\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 11:30:39\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mpost_training_steps\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mPushing model to hub...\u001b[0m\n",
            "adapter_model.safetensors:   0% 0.00/1.21G [00:00<?, ?B/s]\n",
            "tokenizer.model:   0% 0.00/500k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Upload 3 LFS files:   0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin:   0% 0.00/5.37k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin: 100% 5.37k/5.37k [00:00<00:00, 46.9kB/s]\u001b[A\u001b[A\u001b[A\n",
            "training_args.bin: 100% 5.37k/5.37k [00:00<00:00, 14.2kB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 412kB/s] \n",
            "adapter_model.safetensors: 100% 1.21G/1.21G [01:00<00:00, 19.8MB/s]\n",
            "\n",
            "\n",
            "Upload 3 LFS files: 100% 3/3 [01:01<00:00, 20.44s/it]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-24 11:31:49\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 3435\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain llm [<args>]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meLesrS0KKUm",
        "outputId": "baa0a4e3-edc3-4b0d-96f4-3d10f2484a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: args: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"dicomllm\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype='auto'\n",
        ").eval()\n",
        "\n",
        "# Prompt content: \"hi\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"DOB 21.11.1995 IVAN RAMPONI Shoulder CT L\"}\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
        "output_ids = model.generate(input_ids.to('cuda'))\n",
        "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# Model response: \"Hello! How can I assist you today?\"\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RPeUonm8Qmvc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "70ea1c923c8341eb9a26ba31f5907c57",
            "be8b7014e7c64f11a5e53079922b0d24",
            "ae4e139583e843a7bdc89cb06252d34f",
            "835b57942dd049baba06f85d27cda958",
            "070e305242cd43b8aafa46e3546999e4",
            "162af09a7d2a4925824ba0e3222daae8",
            "f740f79e393348adac81468031540207",
            "2946d111665c4a73b572b95b4f70ecd1",
            "3df5b81db37b459cbaa23e5f7ca44531",
            "1a49ae0adfbf4fa0ba86a2110706549f",
            "c584d98873f44bfda729c4da1e9cacdf"
          ]
        },
        "outputId": "96835e69-975d-4a1f-be57-938e5df02d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70ea1c923c8341eb9a26ba31f5907c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32001, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32001, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fd61eb32f709>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_adapter_model_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3827\u001b[0;31m             model.load_adapter(\n\u001b[0m\u001b[1;32m   3828\u001b[0m                 \u001b[0m_adapter_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3829\u001b[0m                 \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# Load state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mincompatible_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_peft_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_adapter_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mincompatible_keys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_model_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     )\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mload_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_prompt_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         model.prompt_encoder[adapter_name].embedding.load_state_dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32001, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32001, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wU6WJcG3V-8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}